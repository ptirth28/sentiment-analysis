{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Clientsubjectivity.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM31y6bTQW7Y2XlgCem/oCM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"x8wIYnp3W46T"},"source":["!wget http://nlp.stanford.edu/data/glove.6B.zip\n","!unzip -q glove.6B.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0yNvqGqBW7p5"},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bmCo2eipW-CR"},"source":["import pandas as pd\n","import numpy as np\n","\n","# text preprocessing\n","from nltk.tokenize import word_tokenize\n","import re\n","\n","# plots and metrics\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n","\n","# preparing input to our model\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical\n","\n","# keras layers\n","from keras.models import Sequential\n","from keras.layers import Embedding, Bidirectional, LSTM, GRU, Dense"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"svyj-4AnXAya"},"source":["num_classes = 2\n","\n","# Number of dimensions for word embedding\n","embed_num_dims = 300\n","\n","# Max input length (max number of words) \n","max_seq_len = 47\n","\n","class_names = ['zero', 'one']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RoPs_gSeXDRT"},"source":["#importing the data file\n","total_data = pd.read_csv(\"preprocessed_data_file.csv\", encoding=\"ISO-8859-1\")\n","total_data['text']=total_data['text'].apply(str)\n","total_data = total_data.dropna()\n","total_data.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sPGS01kZXR8a"},"source":["#splitting the data in test and train set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(total_data['final_text'],total_data['subjectivity'], test_size = 0.20, random_state = 42)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pPlyUbNQXUQE"},"source":["#converting the text data to sequence \n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(total_data['final_text'])\n","\n","\n","sequence_train = tokenizer.texts_to_sequences(X_train)\n","sequence_test = tokenizer.texts_to_sequences(X_test)\n","\n","index_of_words = tokenizer.word_index\n","\n","# vacab size is number of unique words + reserved 0 index for padding\n","vocab_size = len(index_of_words) + 1\n","\n","print('Number of unique words: {}'.format(len(index_of_words)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m81KE2W8gtie"},"source":["import pickle\n","# saving the text tokenizer \n","with open('subjectivity_tokenizer.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"32iGor2oXX4r"},"source":["##For full data\n","seq = tokenizer.texts_to_sequences(total_data['final_text'])\n","data_pad = pad_sequences(seq, maxlen = max_seq_len )\n","data_pad"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kU7A2TcyXZ6m"},"source":["#padding sequence which are less then the max length given\n","X_train_pad = pad_sequences(sequence_train, maxlen = max_seq_len ) ##post padding\n","X_test_pad = pad_sequences(sequence_test, maxlen = max_seq_len )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tnkmhgYaXcMq"},"source":["#Funtion to create an embedding matrix which will contain each word and its respective vector representation \n","def create_embedding_matrix(filepath, word_index, embedding_dim):\n","    vocab_size = len(word_index) + 1  # Adding again 1 because of reserved 0 index\n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","    with open(filepath) as f:\n","        for line in f:\n","            word, *vector = line.split()\n","            if word in word_index:\n","                idx = word_index[word] \n","                embedding_matrix[idx] = np.array(\n","                    vector, dtype=np.float32)[:embedding_dim]\n","    return embedding_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-Hh1FHJDXdqX"},"source":["#Glove file with vector dimenion of 300\n","fname = 'glove.6B.300d.txt'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O6-_yuZRXfXN"},"source":["#creating embedding matrix\n","embedd_matrix = create_embedding_matrix(fname, index_of_words, embed_num_dims)\n","embedd_matrix.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NE9NCGDXXhRL"},"source":["# Inspect unseen words\n","new_words = 0\n","\n","for word in index_of_words:\n","    entry = embedd_matrix[index_of_words[word]]\n","    if all(v == 0 for v in entry):\n","        new_words = new_words + 1\n","\n","print('Words found in wiki vocab: ' + str(len(index_of_words) - new_words))\n","print('New words found: ' + str(new_words))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NO7C_1zwXjSL"},"source":["# Embedding layer before the actaul BLSTM \n","embedd_layer = Embedding(vocab_size,\n","                         embed_num_dims,\n","                         input_length = max_seq_len,\n","                         weights = [embedd_matrix],\n","                         trainable=False)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YTAAPPstXlJY"},"source":["#MODEL architecture\n","# Parameters\n","lstm_output_size = 128\n","bidirectional = True\n","\n","# Embedding Layer, LSTM or biLSTM, Dense, softmax\n","model = Sequential()\n","model.add(embedd_layer)\n","\n","if bidirectional:\n","    model.add(Bidirectional(LSTM(units=lstm_output_size,return_sequences=True)))\n","                             \n","else:\n","     model.add(LSTM(units=lstm_output_size,return_sequences=True))\n","               \n","if bidirectional:\n","    model.add(Bidirectional(LSTM(units=lstm_output_size,dropout=0.2,recurrent_dropout=0.2)))\n","else:\n","    model.add(LSTM(units=lstm_output_size,dropout=0.2,recurrent_dropout=0.2))\n","\n","#model.add(Dense(num_classes, activation='softmax'))\n","model.add(Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d8xsSM-hXmqD"},"source":["#model compilation\n","model.compile(loss = 'mean_squared_error', optimizer = 'adam', metrics = ['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cxZG8qwNXoa0"},"source":["#training\n","batch_size = 1000\n","epochs = 20\n","\n","hist = model.fit(X_train_pad, y_train, \n","                 batch_size=batch_size,\n","                 epochs=epochs,\n","                 validation_data=(X_test_pad,y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FxW47-jTcN-r"},"source":["#predictions\n","predictions = model.predict(data_pad)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yXZxcgkPcYC9"},"source":["#Saving the model\n","from keras.models import load_model\n","model.save('subjectivity_model.h5')"],"execution_count":null,"outputs":[]}]}