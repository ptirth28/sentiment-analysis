{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Bert training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "9m-BAsTpc2O1"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import json, nltk\n",
        "import spacy\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7de2nGHiixcH"
      },
      "source": [
        "\"\"\"nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWIv98ngdJpt"
      },
      "source": [
        "contractions = { \n",
        "\"ain't\": \"am not / are not / is not / has not / have not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is / how does\",\n",
        "\"I'd\": \"I had / I would\",\n",
        "\"I'd've\": \"I would have\",\n",
        "\"I'll\": \"I shall / I will\",\n",
        "\"I'll've\": \"I shall have / I will have\",\n",
        "\"I'm\": \"I am\",\n",
        "\"I've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHdusEQldXDg"
      },
      "source": [
        "total_data = pd.read_csv(\"data.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cZmskGUdXUI"
      },
      "source": [
        "total_data['text']=total_data['text'].apply(str)\n",
        "total_data = total_data.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NbAced6dXXx"
      },
      "source": [
        "!pip install emot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlHHQOPidXbx"
      },
      "source": [
        "from emot.emo_unicode import UNICODE_EMO, EMOTICONS\n",
        "# Converting emojis to words\n",
        "def convert_emojis(text):\n",
        "    for emot in UNICODE_EMO:\n",
        "        text = text.replace(emot, \"_\".join(UNICODE_EMO[emot].replace(\",\",\"\").replace(\":\",\"\").split()))\n",
        "        return text\n",
        "# Converting emoticons to words    \n",
        "def convert_emoticons(text):\n",
        "    for emot in EMOTICONS:\n",
        "        text = re.sub(u'('+emot+')', \"_\".join(EMOTICONS[emot].replace(\",\",\"\").split()), text)\n",
        "        return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49BULW3IdXd5"
      },
      "source": [
        "def process_tweet(tweet):\n",
        "    tweet = tweet.lower()                                             # Lowercases the string\n",
        "    tweet = re.sub('@[^\\s]+', '', tweet)                              # Removes usernames\n",
        "    tweet = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+))', ' ', tweet)   # Remove URLs\n",
        "    tweet = re.sub(r\"\\d+\", \" \", str(tweet))                           # Removes all digits\n",
        "    tweet = re.sub('&quot;',\" \", tweet)                               # Remove (&quot;) \n",
        "    tweet = convert_emojis(tweet)  \n",
        "    tweet = convert_emoticons(tweet)                                             # Replaces Emojis\n",
        "    tweet = re.sub(r\"\\b[a-zA-Z]\\b\", \"\", str(tweet))                   # Removes all single characters\n",
        "    for word in tweet.split():\n",
        "        if word.lower() in contractions:\n",
        "            tweet = tweet.replace(word, contractions[word.lower()])   # Replaces contractions\n",
        "    tweet = re.sub(r\"[^\\w\\s]\", \" \", str(tweet))                       # Removes all punctuations\n",
        "    tweet = re.sub(r'(.)\\1+', r'\\1\\1', tweet)                         # Convert more than 2 letter repetitions to 2 letter\n",
        "    tweet = re.sub(r\"\\s+\", \" \", str(tweet))                           # Replaces double spaces with single space    \n",
        "    return tweet"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV7VhYnvdXhB"
      },
      "source": [
        "total_data['text'] = np.vectorize(process_tweet)(total_data['text'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7k8KvoJFjAwg"
      },
      "source": [
        "\"\"\"#Importing stopwords from nltk library\n",
        "from nltk.corpus import stopwords\n",
        "STOPWORDS = set(stopwords.words('english'))\n",
        "\n",
        "# Function to remove the stopwords\n",
        "def stopwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n",
        "\n",
        "# Applying the stopwords to 'text_punct' and store into 'text_stop'\n",
        "total_data[\"text\"] = total_data[\"text\"].apply(stopwords)\n",
        "total_data[\"text\"].head()\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0SNHEXSjDYz"
      },
      "source": [
        "\"\"\"# Checking the first 10 most frequent words\n",
        "from collections import Counter\n",
        "cnt = Counter()\n",
        "for text in total_data[\"text\"].values:\n",
        "    for word in text.split():\n",
        "        cnt[word] += 1\n",
        "        \n",
        "cnt.most_common(10)\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LRkUSWpejIv5"
      },
      "source": [
        "\"\"\"# Removing the frequent words\n",
        "freq = set([w for (w, wc) in cnt.most_common(10)])\n",
        "\n",
        "# function to remove the frequent words\n",
        "def freqwords(text):\n",
        "    return \" \".join([word for word in str(text).split() if word not in freq])\n",
        "\n",
        "# Passing the function freqwords\n",
        "total_data[\"text\"] = total_data[\"text\"].apply(freqwords)\n",
        "total_data[\"text\"].head()\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geKeFAE9jOPW"
      },
      "source": [
        "\"\"\"# Removal of 10 rare words \n",
        "freq = pd.Series(' '.join(total_data['text']).split()).value_counts()[-10:] # 10 rare words\n",
        "freq = list(freq.index)\n",
        "total_data['text'] = total_data['text'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n",
        "total_data['text'].head()\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXrnzF9kdXj8"
      },
      "source": [
        "#spell check\n",
        "from textblob import TextBlob\n",
        "total_data['text'].apply(lambda x: str(TextBlob(x).correct()))\n",
        "total_data.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7gItbxkEdXnb"
      },
      "source": [
        "!python -m spacy download en_core_web_lg"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "baeX46tDdXqa"
      },
      "source": [
        "import en_core_web_lg\n",
        "# call model\n",
        "ner = en_core_web_lg.load()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JLsyoAKRd6e6"
      },
      "source": [
        "final_text = []\n",
        "for index, row in total_data.iterrows():\n",
        "  text_data = row['text']\n",
        "  t = ner(text_data)\n",
        "  final_text.append(\" \".join([ent.text for ent in t if not ent.ent_type_]))\n",
        "\n",
        "df = pd.DataFrame()\n",
        "df = total_data\n",
        "\n",
        "df['text'] = final_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-davU5PTd6iK"
      },
      "source": [
        "import torch\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J_6yR5Zxd6l1"
      },
      "source": [
        "df = df.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D905BTL0d6o4"
      },
      "source": [
        "def label(df):\n",
        "  if (df['polarity']>=-0.2) and (df['polarity']<=0.2):\n",
        "    return 0\n",
        "  elif (df['polarity']>0.2):\n",
        "    return 1\n",
        "  elif (df['polarity']<-0.2):\n",
        "    return 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8wVFfELfvqn"
      },
      "source": [
        "def label(df):\n",
        "  if (df['subjectivity']>0.5) :\n",
        "    return 1\n",
        "  elif (df['subjectivity']<=0.5) :\n",
        "    return 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HpNM2vnfvwH"
      },
      "source": [
        "df['label'] = df.apply(label, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuwlSQWTfvzH"
      },
      "source": [
        "# Train test split\n",
        "train, test = train_test_split(df, test_size=0.2, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-p-XSl11gIFs"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ap9CAJoQgINh"
      },
      "source": [
        "# Loading tokenizer and encoding our data\n",
        "from transformers import DistilBertTokenizer\n",
        "from torch.utils.data import TensorDataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dx2W2N7TgT5J"
      },
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PzlJXJAPgW4L"
      },
      "source": [
        "encoded_data_train = tokenizer.batch_encode_plus(\n",
        "    train.text.values, \n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True, \n",
        "    max_length=64, \n",
        "    return_tensors='pt'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74oxGVM8gjFz"
      },
      "source": [
        "encoded_data_test = tokenizer.batch_encode_plus(\n",
        "    df.text.values, \n",
        "    add_special_tokens=True, \n",
        "    return_attention_mask=True, \n",
        "    pad_to_max_length=True, \n",
        "    max_length=64, \n",
        "    return_tensors='pt'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cwgHQBsmgnVD"
      },
      "source": [
        "input_ids_train = encoded_data_train['input_ids']\n",
        "attention_masks_train = encoded_data_train['attention_mask']\n",
        "labels_train = torch.tensor(train.label.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unQCz5Degq85"
      },
      "source": [
        "input_ids_test = encoded_data_test['input_ids']\n",
        "attention_masks_test = encoded_data_test['attention_mask']\n",
        "labels_test = torch.tensor(df.label.values)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8kvAC7bgszY"
      },
      "source": [
        "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCbI9weKgs8J"
      },
      "source": [
        "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nEbx_obsgs_a"
      },
      "source": [
        "# BERT pretrained model\n",
        "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertConfig\n",
        "\n",
        "model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=3, output_attentions=False, output_hidden_states=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEI78ABugtC0"
      },
      "source": [
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XL0IUrx7gtGK"
      },
      "source": [
        "# Creating Dataloaders\n",
        "batch_size = 32\n",
        "\n",
        "dataloader_train = DataLoader(dataset_train, \n",
        "                              sampler=RandomSampler(dataset_train), \n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Tur35cKgtJT"
      },
      "source": [
        "batch_size = 8\n",
        "\n",
        "dataloader_test = DataLoader(dataset_test, \n",
        "                              sampler=RandomSampler(dataset_test), \n",
        "                              batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F7lWoYMdgtMc"
      },
      "source": [
        "from transformers import AdamW, get_linear_schedule_with_warmup"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MJLFZUC1gtRb"
      },
      "source": [
        "# Setting up optimizer\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr=5e-5, \n",
        "                  eps=1e-8)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHZOYSGlgtUX"
      },
      "source": [
        "# Setting up scheduler\n",
        "epochs = 2\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(dataloader_train)*epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUDRK-kvhI2v"
      },
      "source": [
        "epochs = 2\n",
        "\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps=0,\n",
        "                                            num_training_steps=len(dataloader_test)*epochs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "330sr1UbheEW"
      },
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q7Q6r0KhI6B"
      },
      "source": [
        "def precision_score_func(labels, preds):\n",
        "  labels_flat = labels.flatten()\n",
        "  preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "  return precision_score(labels_flat, preds_flat, average='weighted')\n",
        "\n",
        "def recall_score_func(labels, preds):\n",
        "  labels_flat = labels.flatten()\n",
        "  preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "  return recall_score(labels_flat, preds_flat, average='weighted')\n",
        "\n",
        "def f1_score_func(labels, preds):\n",
        "  preds_flat = np.argmax(preds, axis=1).flatten()\n",
        "  labels_flat = labels.flatten()\n",
        "  return f1_score(labels_flat, preds_flat, average='weighted')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QIj4eFEehI9U"
      },
      "source": [
        "import random\n",
        "\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zrp1YIfNhJAe"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d3sMQlGJhJEC"
      },
      "source": [
        "def evaluate(dataloader_train):\n",
        "\n",
        "    model.eval()\n",
        "    \n",
        "    loss_val_total = 0\n",
        "    predictions, true_vals = [], []\n",
        "    \n",
        "    for batch in dataloader_train:\n",
        "        \n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }\n",
        "\n",
        "        with torch.no_grad():        \n",
        "            outputs = model(**inputs)\n",
        "            \n",
        "        loss = outputs[0]\n",
        "        logits = outputs[1]\n",
        "        loss_val_total += loss.item()\n",
        "\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = inputs['labels'].cpu().numpy()\n",
        "        predictions.append(logits)\n",
        "        true_vals.append(label_ids)\n",
        "    \n",
        "    loss_train_avg = loss_val_total/len(dataloader_train) \n",
        "    \n",
        "    predictions = np.concatenate(predictions, axis=0)\n",
        "    true_vals = np.concatenate(true_vals, axis=0)\n",
        "            \n",
        "    return loss_train_avg, predictions, true_vals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UDKaTJOjhJHa"
      },
      "source": [
        "# Training loop\n",
        "for epoch in tqdm(range(1, epochs+1)):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    loss_train_total = 0\n",
        "\n",
        "    progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
        "    for batch in progress_bar:\n",
        "\n",
        "        model.zero_grad()\n",
        "        \n",
        "        batch = tuple(b.to(device) for b in batch)\n",
        "        \n",
        "        inputs = {'input_ids':      batch[0],\n",
        "                  'attention_mask': batch[1],\n",
        "                  'labels':         batch[2],\n",
        "                 }       \n",
        "\n",
        "        outputs = model(**inputs)\n",
        "        \n",
        "        loss = outputs[0]\n",
        "        loss_train_total += loss.item()\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        optimizer.step()\n",
        "        scheduler.step()\n",
        "        \n",
        "        progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
        "             \n",
        "    torch.save(model.state_dict(), f'finetuned_BERT_epoch_{epoch}.model')\n",
        "        \n",
        "    tqdm.write(f'\\nEpoch {epoch}')\n",
        "    \n",
        "    loss_train_avg = loss_train_total/len(dataloader_train)             \n",
        "    tqdm.write(f'Training loss: {loss_train_avg}')\n",
        "    \n",
        "    train_loss, predictions, true_vals = evaluate(dataloader_train)\n",
        "    train_precision = precision_score_func(true_vals, predictions)\n",
        "    train_recall = recall_score_func(true_vals, predictions)\n",
        "    train_f1 = f1_score_func(true_vals, predictions)\n",
        "    tqdm.write(f'Train loss: {train_loss}')\n",
        "    tqdm.write(f'Precision Score (weighted): {train_precision}')\n",
        "    tqdm.write(f'Recall Score (Weighted): {train_recall}')\n",
        "    tqdm.write(f'F1 Score (Weighted): {train_f1}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5xUvrRXh1Eq"
      },
      "source": [
        "model.load_state_dict(torch.load('model_name.model', map_location=torch.device('cuda')))\n",
        "model = model.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M29FAzOEiZFu"
      },
      "source": [
        "_, predictions, true_vals = evaluate(dataloader_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJPj6gEkiZQJ"
      },
      "source": [
        "_, predictions, true_vals = evaluate(dataloader_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziuFZFpLiZUw"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "y_true = true_vals.flatten()\n",
        "y_pred = np.argmax(predictions, axis=1).flatten()\n",
        "print(metrics.confusion_matrix(y_true, y_pred))\n",
        "print(metrics.classification_report(y_true, y_pred, digits=2))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}